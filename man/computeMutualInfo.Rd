% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/computeInformation.R
\name{computeMutualInfo}
\alias{computeMutualInfo}
\title{Compute (conditional) mutual information}
\usage{
computeMutualInfo(
  X,
  Y,
  df_conditioning = NULL,
  maxbins = NULL,
  cplx = c("nml", "mdl"),
  n_eff = -1,
  sample_weights = NULL,
  is_continuous = NULL,
  plot = FALSE
)
}
\arguments{
\item{X}{[a vector]
A vector that contains the observational data of the first variable.}

\item{Y}{[a vector]
A vector that contains the observational data of the second variable.}

\item{df_conditioning}{[a data frame]
The data frame of the observations of the conditioning variables.}

\item{maxbins}{[an integer]
When the data contain continuous variables, the maximum number of bins
allowed during the discretization. A smaller number makes the computation
faster, a larger number allows finer discretization.}

\item{cplx}{[a string]
The complexity model:
\itemize{
\item["mdl"] Minimum description Length
\item["nml"] Normalized Maximum Likelihood, less costly compared to "mdl" in
the finite sample case and will allow for more bins.
}}

\item{n_eff}{[an integer]
The number of effective samples. When there is significant autocorrelation in
the samples you may want to specify a number of effective samples that is
lower than the number of points in the distribution.}

\item{sample_weights}{[a vector of floats]
Individual weights for each sample, used for the same reason as the effective
sample number but with individual precision.}

\item{is_continuous}{[a vector of booleans]
Specify if each variable is to be treated as continuous (TRUE) or discrete
(FALSE), must be of length `ncol(df_conditioning) + 2`, in the order
\eqn{X, Y, U1, U2, ...}. If not specified, factors and character vectors are
considered as discrete, and numerical vectors as continuous.}

\item{plot}{[a boolean]
Specify whether the XY joint space with discretization scheme is to be
plotted (requires `ggplot2` and `gridExtra`).}
}
\value{
A list that contains :
\itemize{
\item cutpoints1: Only when \eqn{X} is continuous, a vector containing
  the cutpoints for the partitioning of \eqn{X}.
\item cutpoints2: Only when \eqn{Y} is continuous, a vector containing
  the cutpoints for the partitioning of \eqn{Y}.
\item niterations: Only when at least one of the input variables is
  continuous, the number of iterations it takes to reach the convergence of
  the estimated information.
\item iterationN: Only when at least one of the input variables is
  continuous, the list of vectors of cutpoints of each iteration.
\item info: The estimation of (conditional) mutual information without the
complexity cost.
\item infok: The estimation of (conditional) mutual information with the
complexity cost (\eqn{Ik = I - cplx}).
\item plot: Only when `plot == TRUE`, the plot object.
}
}
\description{
For discrete variables, the computation is based on the
empirical frequency minus a complexity cost (computed as BIC or with the
Normalized Maximum Likelihood). When continuous variables are present, each
continuous variable is discretized where the partitioning is chosen by
maximizing the mutual information minus the complexity cost. The estimation
based on the optimally discretized distributions effectively approaches the
mutual information computed on the original continuous variables.
}
\details{
For a pair of continuous variables \eqn{X} and \eqn{Y}, the mutual
information \eqn{I(X;Y)} will be computed iteratively. In each iteration, the
algorithm optimizes first the partitioning of \eqn{X} and then that of
\eqn{Y}, while maximizing
\deqn{Ik(X_{d};Y_{d}) = I(X_{d};Y_{d}) - cplx(X_{d};Y_{d})}
where \eqn{cplx(X_{d}; Y_{d})} is the complexity cost of the current
partitioning (see Affeldt 2016 and Cabeli 2020). Upon convergence, the
information terms \eqn{I(X_{d};Y_{d})} and \eqn{Ik(X_{d};Y_{d})}, as well as
the partitioning of \eqn{X_{d}} and \eqn{Y_{d}} in terms of cutpoints, are
returned.

For conditional mutual information with conditioning set \eqn{U}, the
computation is done based on
\deqn{
  Ik(X;Y|U) = 0.5*(Ik(X_{d};Y_{d},U_{d}) - Ik(X_{d};U_{d})
                 + Ik(Y_{d};X_{d},U_{d}) - Ik(Y_{d};U_{d})),
}
where each of the four summands is estimated independently.
}
\examples{
library(miic)
N <- 1000
# Dependence, conditional independence : X <- Z -> Y
Z <- runif(N)
X <- Z * 2 + rnorm(N, sd = 0.2)
Y <- Z * 2 + rnorm(N, sd = 0.2)
res <- computeMutualInfo(X, Y, plot = FALSE)
message("I(X;Y) = ", res$info)
res <- computeMutualInfo(X, Y, df_conditioning = matrix(Z, ncol = 1), plot = FALSE)
message("I(X;Y|Z) = ", res$info)

\donttest{
# Conditional independence with categorical conditioning variable : X <- Z -> Y
Z <- sample(1:3, N, replace = TRUE)
X <- -as.numeric(Z == 1) + as.numeric(Z == 2) + 0.2 * rnorm(N)
Y <- as.numeric(Z == 1) + as.numeric(Z == 2) + 0.2 * rnorm(N)
res <- miic::computeMutualInfo(X, Y, cplx = "nml")
message("I(X;Y) = ", res$info)
res <- miic::computeMutualInfo(X, Y, matrix(Z, ncol = 1), is_continuous = c(TRUE, TRUE, FALSE))
message("I(X;Y|Z) = ", res$info)


# Independence, conditional dependence : X -> Z <- Y
X <- runif(N)
Y <- runif(N)
Z <- X + Y + rnorm(N, sd = 0.1)
res <- computeMutualInfo(X, Y, plot = TRUE)
message("I(X;Y) = ", res$info)
res <- computeMutualInfo(X, Y, df_conditioning = matrix(Z, ncol = 1), plot = TRUE)
message("I(X;Y|Z) = ", res$info)
}

}
\references{
\itemize{
\item Verny et al., \emph{PLoS Comp. Bio. 2017.}
  https://doi.org/10.1371/journal.pcbi.1005662
\item Cabeli et al., \emph{PLoS Comp. Bio. 2020.}
  https://doi.org/10.1371/journal.pcbi.1007866
\item Affeldt et al., \emph{Bioinformatics 2016}
}
}
